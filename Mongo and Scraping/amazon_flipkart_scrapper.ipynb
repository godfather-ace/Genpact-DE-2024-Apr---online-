{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard header for webpage agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen #used to open urls\n",
    "from bs4 import BeautifulSoup #used for extracting data from html websites\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking Amazon product URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.amazon.in/Sony-PS5-Console-Modern-Warfare/dp/B0CMQPPMB1\"\n",
    "webpage = requests.get(URL, headers = HEADERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a soup with lxml parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(webpage.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting product title and cleaning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"a-size-large product-title-word-break\" id=\"productTitle\">        Sony PS5 Console - Call of Duty Modern Warfare III Bundle       </span>\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(\"span\", attrs = {\"id\": \"productTitle\"})\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sony PS5 Console - Call of Duty Modern Warfare III Bundle       \n"
     ]
    }
   ],
   "source": [
    "title_value = title.string\n",
    "print(title_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sony PS5 Console - Call of Duty Modern Warfare III Bundle\n"
     ]
    }
   ],
   "source": [
    "title_string = title_value.strip()\n",
    "print(title_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the price of the product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "₹55,390.00\n"
     ]
    }
   ],
   "source": [
    "price = soup.find(\"span\", attrs = {\"class\": \"a-offscreen\"}).string.strip()\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the rating star of the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5 out of 5 stars\n"
     ]
    }
   ],
   "source": [
    "rating_star = soup.find(\"i\", attrs = {\"class\": \"a-icon a-icon-star a-star-4-5 cm-cr-review-stars-spacing-big\"}).string\n",
    "print(rating_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the rating count of the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 ratings\n"
     ]
    }
   ],
   "source": [
    "rating_count = soup.find(\"span\", attrs = {\"id\": \"acrCustomerReviewText\"}).string\n",
    "print(rating_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the product is available or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In stock \n"
     ]
    }
   ],
   "source": [
    "product_availability = soup.find(\"div\", attrs = {\"id\": \"availability\"})\n",
    "product_availability = product_availability.find(\"span\").string\n",
    "print(product_availability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling all the codes in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Title: Sony PS5 Console\n",
      "***********************************\n",
      "Product Price: ₹54,990.00\n",
      "***********************************\n",
      "product Rating Star: 4.6 out of 5 stars\n",
      "***********************************\n",
      "Product Rating Count: 1,403 ratings\n",
      "***********************************\n",
      "Product Availability:   Only 1 left in stock. \n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen #used to open urls\n",
    "from bs4 import BeautifulSoup #used for extracting data from html websites\n",
    "import requests\n",
    "\n",
    "def get_title(soup): \n",
    "    try: \n",
    "        title = soup.find(\"span\", attrs = {\"id\": \"productTitle\"})\n",
    "        #print(title)\n",
    "        title_value = title.string\n",
    "        #print(title_value)\n",
    "        title_string = title_value.strip()\n",
    "    except AttributeError: \n",
    "        title_string = \"NA\"\n",
    "    return title_string\n",
    "\n",
    "def get_price(soup): \n",
    "    try: \n",
    "        price = soup.find(\"span\", attrs = {\"class\": \"a-offscreen\"}).string.strip()\n",
    "    except AttributeError: \n",
    "        price = \"NA\"\n",
    "    return price\n",
    "\n",
    "def get_rating_star(soup):\n",
    "    try: \n",
    "        rating_star = soup.find(\"i\", attrs = {\"class\": \"a-icon a-icon-star a-star-4-5 cm-cr-review-stars-spacing-big\"}).string\n",
    "    except AttributeError: \n",
    "        rating_star = \"NA\"\n",
    "    return rating_star\n",
    "\n",
    "def get_rating_count(soup):\n",
    "    try: \n",
    "        rating_count = soup.find(\"span\", attrs = {\"id\": \"acrCustomerReviewText\"}).string\n",
    "    except AttributeError:\n",
    "        rating_count = \"NA\"\n",
    "    return rating_count\n",
    "\n",
    "def get_availability(soup):\n",
    "    try: \n",
    "        product_availability = soup.find(\"div\", attrs = {\"id\": \"availability\"})\n",
    "        product_availability = product_availability.find(\"span\").string\n",
    "    except AttributeError: \n",
    "        product_availability = \"NA\"\n",
    "    return product_availability\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    # Headers for request\n",
    "    #HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    # Webpage URL\n",
    "    URL = input(\"Enter an amazon product URL\")\n",
    "    #URL = \"https://www.amazon.in/Sony-PS5-Console-Modern-Warfare/dp/B0CMQPPMB1\"\n",
    "    #webpage = requests.get(URL)\n",
    "    webpage = requests.get(URL, headers = HEADERS)\n",
    "\n",
    "    # Soup object containing webpage dat+a\n",
    "    soup = BeautifulSoup(webpage.content, 'lxml')\n",
    "    print(\"Product Title:\", get_title(soup))\n",
    "    print(\"***********************************\")\n",
    "    print(\"Product Price:\", get_price(soup))\n",
    "    print(\"***********************************\")\n",
    "    print(\"product Rating Star:\", get_rating_star(soup))\n",
    "    print(\"***********************************\")\n",
    "    print(\"Product Rating Count:\", get_rating_count(soup))\n",
    "    print(\"***********************************\")\n",
    "    print(\"Product Availability: \", get_availability(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking multiple products based on amazon search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipkart Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Title: NA\n",
      "***********************************\n",
      "Product Price: NA\n",
      "***********************************\n",
      "product Rating Star: None\n",
      "***********************************\n",
      "Product Rating Count: &\n",
      "***********************************\n",
      "Product Title: NA\n",
      "***********************************\n",
      "Product Price: NA\n",
      "***********************************\n",
      "product Rating Star: None\n",
      "***********************************\n",
      "Product Rating Count: &\n",
      "***********************************\n",
      "Product Title: NA\n",
      "***********************************\n",
      "Product Price: NA\n",
      "***********************************\n",
      "product Rating Star: None\n",
      "***********************************\n",
      "Product Rating Count: &\n",
      "***********************************\n",
      "Product Title: NA\n",
      "***********************************\n",
      "Product Price: NA\n",
      "***********************************\n",
      "product Rating Star: None\n",
      "***********************************\n",
      "Product Rating Count: &\n",
      "***********************************\n",
      "Product Title: NA\n",
      "***********************************\n",
      "Product Price: NA\n",
      "***********************************\n",
      "product Rating Star: None\n",
      "***********************************\n",
      "Product Rating Count: &\n",
      "***********************************\n",
      "Product Title: NA\n",
      "***********************************\n",
      "Product Price: NA\n",
      "***********************************\n",
      "product Rating Star: None\n",
      "***********************************\n",
      "Product Rating Count: &\n",
      "***********************************\n",
      "Product Title: NA\n",
      "***********************************\n",
      "Product Price: NA\n",
      "***********************************\n",
      "product Rating Star: None\n",
      "***********************************\n",
      "Product Rating Count: &\n",
      "***********************************\n",
      "Product Title: NA\n",
      "***********************************\n",
      "Product Price: NA\n",
      "***********************************\n",
      "product Rating Star: None\n",
      "***********************************\n",
      "Product Rating Count: &\n",
      "***********************************\n",
      "Product Title: NA\n",
      "***********************************\n",
      "Product Price: NA\n",
      "***********************************\n",
      "product Rating Star: None\n",
      "***********************************\n",
      "Product Rating Count: &\n",
      "***********************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m     links_list\u001b[38;5;241m.\u001b[39mappend(link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links_list:\n\u001b[1;32m---> 63\u001b[0m     new_webpage \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.flipkart.com\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     new_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(new_webpage\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct Title:\u001b[39m\u001b[38;5;124m\"\u001b[39m, get_title(new_soup))\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1390\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1390\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1392\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen #used to open urls\n",
    "from bs4 import BeautifulSoup #used for extracting data from html websites\n",
    "import requests\n",
    "\n",
    "def get_title(soup): \n",
    "    try: \n",
    "        title = soup.find(\"div\", attrs = {\"class\": \"_4rR01T\"})\n",
    "        #print(title)\n",
    "        title_value = title.string\n",
    "        #print(title_value)\n",
    "        title_string = title_value.strip()\n",
    "    except AttributeError: \n",
    "        title_string = \"NA\"\n",
    "    return title_string\n",
    "\n",
    "def get_price(soup): \n",
    "    try: \n",
    "        price = soup.find(\"div\", attrs = {\"class\": \"_30jeq3 _1_WHN1\"}).string.strip()\n",
    "    except AttributeError: \n",
    "        price = \"NA\"\n",
    "    return price\n",
    "\n",
    "def get_rating_star(soup):\n",
    "    try: \n",
    "        rating_star = soup.find(\"div\", attrs = {\"class\": \"_3LWZlK\"}).string\n",
    "    except AttributeError: \n",
    "        rating_star = \"NA\"\n",
    "    return rating_star\n",
    "\n",
    "def get_rating_count(soup):\n",
    "    try: \n",
    "        rating_count = soup.find(\"span\", attrs = {\"class\": \"_13vcmD\"}).string\n",
    "    except AttributeError:\n",
    "        rating_count = \"NA\"\n",
    "    return rating_count\n",
    "\n",
    "'''def get_availability(soup):\n",
    "    try: \n",
    "        product_availability = soup.find(\"div\", attrs = {\"id\": \"availability\"})\n",
    "        product_availability = product_availability.find(\"span\").string\n",
    "    except AttributeError: \n",
    "        product_availability = \"NA\"\n",
    "    return product_availability'''\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    #Headers for request\n",
    "    HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    # Webpage URL\n",
    "    URL = input(\"Enter flipkar search URL\")\n",
    "    webpage = requests.get(URL, headers = HEADERS)\n",
    "\n",
    "    # Soup object containing webpage dat+a\n",
    "    soup = BeautifulSoup(webpage.content, 'lxml')\n",
    "    \n",
    "    # Fetching the links based on the tag objects\n",
    "    links = soup.find_all(\"a\", attrs = {\"class\": \"_1fQZEK\"})\n",
    "    links_list = []\n",
    "    for link in links: \n",
    "        links_list.append(link.get('href'))\n",
    "    \n",
    "    for link in links_list:\n",
    "        new_webpage = requests.get(\"https://www.flipkart.com\" + link, headers = HEADERS)\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"lxml\")\n",
    "    \n",
    "        print(\"Product Title:\", get_title(new_soup))\n",
    "        print(\"***********************************\")\n",
    "        print(\"Product Price:\", get_price(new_soup))\n",
    "        print(\"***********************************\")\n",
    "        print(\"product Rating Star:\", get_rating_star(new_soup))\n",
    "        print(\"***********************************\")\n",
    "        print(\"Product Rating Count:\", get_rating_count(new_soup))\n",
    "        print(\"***********************************\")\n",
    "        #print(\"Product Availability: \", get_availability(new_soup))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Search Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\ttry:\n",
    "\t\ttitle = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "\t\ttitle_value = title.string\n",
    "\t\ttitle_string = title_value.strip()\n",
    "\texcept AttributeError:\n",
    "\t\ttitle_string = \"\"\t\n",
    "\treturn title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "\ttry:\n",
    "\t\tprice = soup.find(\"span\", attrs={'id':'priceblock_ourprice'}).string.strip()\n",
    "\texcept AttributeError:\n",
    "\t\ttry:\n",
    "\t\t\t# If there is some deal price\n",
    "\t\t\tprice = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    "\t\texcept:\t\t\n",
    "\t\t\tprice = \"\"\t\n",
    "\treturn price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\ttry:\n",
    "\t\trating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "\texcept AttributeError:\n",
    "\t\ttry:\n",
    "\t\t\trating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "\t\texcept:\n",
    "\t\t\trating = \"\"\t\n",
    "\treturn rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "\ttry:\n",
    "\t\treview_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\texcept AttributeError:\n",
    "\t\treview_count = \"\"\t\n",
    "\treturn review_count\n",
    "\n",
    "# Function to extract Availability Status\n",
    "def get_availability(soup):\n",
    "\ttry:\n",
    "\t\tavailable = soup.find(\"div\", attrs={'id':'availability'})\n",
    "\t\tavailable = available.find(\"span\").string.strip()\n",
    "\texcept AttributeError:\n",
    "\t\tavailable = \"Not Available\"\t\n",
    "\treturn available\t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\t# Headers for request\n",
    "\tHEADERS = ({'User-Agent':\n",
    "\t            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "\t            'Accept-Language': 'en-US'})\n",
    "\n",
    "\t# The webpage URL\n",
    "\tURL = \"https://www.amazon.com/s?k=playstation+4&ref=nb_sb_noss_2\"\n",
    "\t\n",
    "\t# HTTP Request\n",
    "\twebpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "\t# Soup Object containing all data\n",
    "\tsoup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "\t# Fetch links as List of Tag Objects\n",
    "\tlinks = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
    "\n",
    "\t# Store the links\n",
    "\tlinks_list = []\n",
    "\n",
    "\t# Loop for extracting links from Tag Objects\n",
    "\tfor link in links:\n",
    "\t\tlinks_list.append(link.get('href'))\n",
    "\n",
    "\n",
    "\t# Loop for extracting product details from each link \n",
    "\tfor link in links_list:\n",
    "\n",
    "\t\tnew_webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
    "\n",
    "\t\tnew_soup = BeautifulSoup(new_webpage.content, \"lxml\")\n",
    "\t\t\n",
    "\t\t# Function calls to display all necessary product information\n",
    "\t\tprint(\"Product Title =\", get_title(new_soup))\n",
    "\t\tprint(\"Product Price =\", get_price(new_soup))\n",
    "\t\tprint(\"Product Rating =\", get_rating(new_soup))\n",
    "\t\tprint(\"Number of Product Reviews =\", get_review_count(new_soup))\n",
    "\t\tprint(\"Availability =\", get_availability(new_soup))\n",
    "\t\tprint()\n",
    "\t\tprint(\"******************************************************\")\n",
    "\t\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
